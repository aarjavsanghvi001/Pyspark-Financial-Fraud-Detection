{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70543a53",
   "metadata": {},
   "source": [
    "### Starting a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02f176e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb4ff299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/30 00:05:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://aarjavs-mbp.attlocal.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FinancialFraudDetection</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1053eda50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('FinancialFraudDetection').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243c1c6",
   "metadata": {},
   "source": [
    "### Creating a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54c69602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a dataframe\n",
    "df_pyspark = spark.read.csv(r'/Users/aarjavsanghvi/Documents/Dataset/Financial Data for Fraud Detection.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83dd4c",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1532b434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|    type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|   1| PAYMENT|  9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
      "|   1|TRANSFER|    181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT|    181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|      1|             0|\n",
      "|   1| PAYMENT| 11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  7817.71|  C90045638|      53860.0|      46042.29| M573487274|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  7107.77| C154988899|     183195.0|     176087.23| M408069119|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  7861.64|C1912850431|    176087.23|     168225.59| M633326333|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  4024.36|C1265012928|       2671.0|           0.0|M1176932104|           0.0|           0.0|      0|             0|\n",
      "|   1|   DEBIT|  5337.77| C712410124|      41720.0|      36382.23| C195600860|       41898.0|      40348.79|      0|             0|\n",
      "|   1|   DEBIT|  9644.94|C1900366749|       4465.0|           0.0| C997608398|       10845.0|     157982.12|      0|             0|\n",
      "|   1| PAYMENT|  3099.97| C249177573|      20771.0|      17671.03|M2096539129|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  2560.74|C1648232591|       5070.0|       2509.26| M972865270|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT| 11633.76|C1716932897|      10127.0|           0.0| M801569151|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  4098.78|C1026483832|     503264.0|     499165.22|M1635378213|           0.0|           0.0|      0|             0|\n",
      "|   1|CASH_OUT|229133.94| C905080434|      15325.0|           0.0| C476402209|        5083.0|      51513.44|      0|             0|\n",
      "|   1| PAYMENT|  1563.82| C761750706|        450.0|           0.0|M1731217984|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|  1157.86|C1237762639|      21156.0|      19998.14|M1877062907|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT|   671.64|C2033524545|      15123.0|      14451.36| M473053293|           0.0|           0.0|      0|             0|\n",
      "|   1|TRANSFER| 215310.3|C1670993182|        705.0|           0.0|C1100439041|       22425.0|           0.0|      0|             0|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the dataset\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d76c7f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|    type|   amount|\n",
      "+--------+---------+\n",
      "| PAYMENT|  9839.64|\n",
      "| PAYMENT|  1864.28|\n",
      "|TRANSFER|    181.0|\n",
      "|CASH_OUT|    181.0|\n",
      "| PAYMENT| 11668.14|\n",
      "| PAYMENT|  7817.71|\n",
      "| PAYMENT|  7107.77|\n",
      "| PAYMENT|  7861.64|\n",
      "| PAYMENT|  4024.36|\n",
      "|   DEBIT|  5337.77|\n",
      "|   DEBIT|  9644.94|\n",
      "| PAYMENT|  3099.97|\n",
      "| PAYMENT|  2560.74|\n",
      "| PAYMENT| 11633.76|\n",
      "| PAYMENT|  4098.78|\n",
      "|CASH_OUT|229133.94|\n",
      "| PAYMENT|  1563.82|\n",
      "| PAYMENT|  1157.86|\n",
      "| PAYMENT|   671.64|\n",
      "|TRANSFER| 215310.3|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print specific columns in the dataset\n",
    "df_pyspark.select(['type','amount']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a781142b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      " |-- isFlaggedFraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b5ad7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('step', 'int'),\n",
       " ('type', 'string'),\n",
       " ('amount', 'double'),\n",
       " ('nameOrig', 'string'),\n",
       " ('oldbalanceOrg', 'double'),\n",
       " ('newbalanceOrig', 'double'),\n",
       " ('nameDest', 'string'),\n",
       " ('oldbalanceDest', 'double'),\n",
       " ('newbalanceDest', 'double'),\n",
       " ('isFraud', 'int'),\n",
       " ('isFlaggedFraud', 'int')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datatype\n",
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7025533d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/30 00:05:25 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 4:==============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------+-----------------+-----------+-----------------+-----------------+-----------+-----------------+------------------+--------------------+--------------------+\n",
      "|summary|              step|    type|           amount|   nameOrig|    oldbalanceOrg|   newbalanceOrig|   nameDest|   oldbalanceDest|    newbalanceDest|             isFraud|      isFlaggedFraud|\n",
      "+-------+------------------+--------+-----------------+-----------+-----------------+-----------------+-----------+-----------------+------------------+--------------------+--------------------+\n",
      "|  count|           6362620| 6362620|          6362620|    6362620|          6362620|          6362620|    6362620|          6362620|           6362620|             6362620|             6362620|\n",
      "|   mean|243.39724563151657|    NULL|179861.9035491309|       NULL|833883.1040744886|855113.6685785917|       NULL|1100701.666519643| 1224996.398201916|0.001290820448180152| 2.51468734577894E-6|\n",
      "| stddev| 142.3319710491291|    NULL|603858.2314629284|       NULL|2888242.673037544|2924048.502954259|       NULL|3399180.112994486|3674128.9421196207| 0.03590479680160414|0.001585774705736...|\n",
      "|    min|                 1| CASH_IN|              0.0|C1000000639|              0.0|              0.0|C1000004082|              0.0|               0.0|                   0|                   0|\n",
      "|    max|               743|TRANSFER|    9.244551664E7| C999999784|    5.958504037E7|    4.958504037E7| M999999784|   3.5601588935E8|    3.5617927892E8|                   1|                   1|\n",
      "+-------+------------------+--------+-----------------+-----------+-----------------+-----------------+-----------+-----------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Describe the dataset\n",
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b08f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 6362620\n",
      "Number of Columns: 11\n"
     ]
    }
   ],
   "source": [
    "# Printing the shape of the dataframe\n",
    "row_count = df_pyspark.count()\n",
    "column_count = len(df_pyspark.columns)\n",
    "\n",
    "print(\"Number of Rows:\", row_count)\n",
    "print(\"Number of Columns:\", column_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf19c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "|step|type|amount|nameOrig|oldbalanceOrg|newbalanceOrig|nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "|   0|   0|     0|       0|            0|             0|       0|             0|             0|      0|             0|\n",
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:=============================================>           (8 + 2) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count nulls for each column\n",
    "null_counts = df_pyspark.select([sum(col(c).isNull().cast('int')).alias(c) for c in df_pyspark.columns])\n",
    "\n",
    "# Show the results\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68dd3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns\n",
    "df = df_pyspark.drop('nameOrig','nameDest','isFlaggedFraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24bc471b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values for 'type':\n",
      "TRANSFER\n",
      "CASH_IN\n",
      "CASH_OUT\n",
      "PAYMENT\n",
      "DEBIT\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:==================================>                      (6 + 4) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# List distinct values for each categorical variable\n",
    "categorical_columns = [col_name for col_name, data_type in df.dtypes if data_type == 'string']\n",
    "\n",
    "distinct_values = {}\n",
    "for col_name in categorical_columns:\n",
    "    distinct_values[col_name] = [row[col_name] for row in df.select(col_name).distinct().collect()]\n",
    "\n",
    "# Print the distinct values\n",
    "for col_name, values in distinct_values.items():\n",
    "    print(f\"Distinct values for '{col_name}':\")\n",
    "    for value in values:\n",
    "        print(value)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba1ce8f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|    type|  count|\n",
      "+--------+-------+\n",
      "|TRANSFER| 532909|\n",
      "| CASH_IN|1399284|\n",
      "|CASH_OUT|2237500|\n",
      "| PAYMENT|2151495|\n",
      "|   DEBIT|  41432|\n",
      "+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupby('type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49c6b04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 19:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|isfraud|  count|\n",
      "+-------+-------+\n",
      "|      1|   8213|\n",
      "|      0|6354407|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupby('isfraud').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94395d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Preprocessing steps\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# create object of StringIndexer class and specify input and output column\n",
    "SI_type = StringIndexer(inputCol='type',outputCol='type_Index')\n",
    "\n",
    "# transform the data\n",
    "df = SI_type.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96234e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|    type|type_Index|\n",
      "+--------+----------+\n",
      "| PAYMENT|       1.0|\n",
      "| PAYMENT|       1.0|\n",
      "|TRANSFER|       3.0|\n",
      "|CASH_OUT|       0.0|\n",
      "| PAYMENT|       1.0|\n",
      "| PAYMENT|       1.0|\n",
      "| PAYMENT|       1.0|\n",
      "| PAYMENT|       1.0|\n",
      "| PAYMENT|       1.0|\n",
      "|   DEBIT|       4.0|\n",
      "|   DEBIT|       4.0|\n",
      "| PAYMENT|       1.0|\n",
      "| PAYMENT|       1.0|\n",
      "| PAYMENT|       1.0|\n",
      "| PAYMENT|       1.0|\n",
      "|CASH_OUT|       0.0|\n",
      "| PAYMENT|       1.0|\n",
      "| PAYMENT|       1.0|\n",
      "| PAYMENT|       1.0|\n",
      "|TRANSFER|       3.0|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('type','type_Index').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1691cf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=36178Kb max_used=36202Kb free=94893Kb\n",
      " bounds [0x00000001020e8000, 0x0000000104478000, 0x000000010a0e8000]\n",
      " total_blobs=13369 nmethods=12378 adapters=903\n",
      " compilation: disabled (not enough contiguous free space left)\n",
      "+--------+----------+-------------+\n",
      "|    type|type_Index|     type_OHE|\n",
      "+--------+----------+-------------+\n",
      "| PAYMENT|       1.0|(4,[1],[1.0])|\n",
      "| PAYMENT|       1.0|(4,[1],[1.0])|\n",
      "|TRANSFER|       3.0|(4,[3],[1.0])|\n",
      "|CASH_OUT|       0.0|(4,[0],[1.0])|\n",
      "| PAYMENT|       1.0|(4,[1],[1.0])|\n",
      "| PAYMENT|       1.0|(4,[1],[1.0])|\n",
      "| PAYMENT|       1.0|(4,[1],[1.0])|\n",
      "| PAYMENT|       1.0|(4,[1],[1.0])|\n",
      "| PAYMENT|       1.0|(4,[1],[1.0])|\n",
      "|   DEBIT|       4.0|    (4,[],[])|\n",
      "|   DEBIT|       4.0|    (4,[],[])|\n",
      "| PAYMENT|       1.0|(4,[1],[1.0])|\n",
      "| PAYMENT|       1.0|(4,[1],[1.0])|\n",
      "| PAYMENT|       1.0|(4,[1],[1.0])|\n",
      "| PAYMENT|       1.0|(4,[1],[1.0])|\n",
      "|CASH_OUT|       0.0|(4,[0],[1.0])|\n",
      "| PAYMENT|       1.0|(4,[1],[1.0])|\n",
      "| PAYMENT|       1.0|(4,[1],[1.0])|\n",
      "| PAYMENT|       1.0|(4,[1],[1.0])|\n",
      "|TRANSFER|       3.0|(4,[3],[1.0])|\n",
      "+--------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    }
   ],
   "source": [
    "# create object and specify input and output column\n",
    "OHE = OneHotEncoder(inputCols=['type_Index'],outputCols=['type_OHE'])\n",
    "\n",
    "# transform the data\n",
    "df = OHE.fit(df).transform(df)\n",
    "\n",
    "df.select('type','type_Index','type_OHE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f56633bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('step', 'int'),\n",
       " ('type', 'string'),\n",
       " ('amount', 'double'),\n",
       " ('oldbalanceOrg', 'double'),\n",
       " ('newbalanceOrig', 'double'),\n",
       " ('oldbalanceDest', 'double'),\n",
       " ('newbalanceDest', 'double'),\n",
       " ('isFraud', 'int'),\n",
       " ('type_Index', 'double'),\n",
       " ('type_OHE', 'vector')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e26a3",
   "metadata": {},
   "source": [
    "### Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "488cce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import lit, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "107349d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the input and output columns of the vector assembler\n",
    "assembler = VectorAssembler(inputCols=['amount',\n",
    "                                      'oldbalanceOrg',\n",
    "                                      'newbalanceOrig',\n",
    "                                      'oldbalanceDest',\n",
    "                                      'newbalanceDest',\n",
    "                                      'type_OHE'],\n",
    "                           outputCol='features')\n",
    "\n",
    "# fill the null values\n",
    "df = df.fillna(0)\n",
    "\n",
    "# transform the data\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b18f3bd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|isfraud|\n",
      "+--------------------+-------+\n",
      "|(9,[0,1,2,6],[983...|      0|\n",
      "|(9,[0,1,2,6],[186...|      0|\n",
      "|(9,[0,1,8],[181.0...|      1|\n",
      "|(9,[0,1,3,5],[181...|      1|\n",
      "|(9,[0,1,2,6],[116...|      0|\n",
      "|(9,[0,1,2,6],[781...|      0|\n",
      "|(9,[0,1,2,6],[710...|      0|\n",
      "|(9,[0,1,2,6],[786...|      0|\n",
      "|(9,[0,1,6],[4024....|      0|\n",
      "|[5337.77,41720.0,...|      0|\n",
      "|(9,[0,1,3,4],[964...|      0|\n",
      "|(9,[0,1,2,6],[309...|      0|\n",
      "|(9,[0,1,2,6],[256...|      0|\n",
      "|(9,[0,1,6],[11633...|      0|\n",
      "|(9,[0,1,2,6],[409...|      0|\n",
      "|[229133.94,15325....|      0|\n",
      "|(9,[0,1,6],[1563....|      0|\n",
      "|(9,[0,1,2,6],[115...|      0|\n",
      "|(9,[0,1,2,6],[671...|      0|\n",
      "|(9,[0,1,3,8],[215...|      0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the transformed vector\n",
    "df.select('features','isfraud').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3530174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model_Dataframe\n",
    "model_df = df.select(['features','isfraud'])\n",
    "model_df = model_df.withColumnRenamed(\"isfraud\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "698c1a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate class weights\n",
    "class_counts = model_df.groupBy('label').count().collect()\n",
    "total_count = model_df.count()\n",
    "class_weights = {row['label']: total_count / row['count'] for row in class_counts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e21645fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training & testing Dataframe\n",
    "training_df,test_df = model_df.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db18e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column with class weights\n",
    "df_weighted = training_df.withColumn('classWeight', lit(1.0))  # default weight for majority class\n",
    "for label, weight in class_weights.items():\n",
    "    df_weighted = df_weighted.withColumn('classWeight', when(df_weighted['label'] == label, weight).otherwise(df_weighted['classWeight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eafaf70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/30 00:05:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:05:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 34:===========>                                             (2 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|label|      classWeight|\n",
      "+-----+-----------------+\n",
      "|    0|1.001292488819177|\n",
      "|    1|774.7010836478753|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 34:============================>                            (5 + 5) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assuming 'df_weighted' is the DataFrame with added class weights\n",
    "df_weighted.select('label', 'classWeight').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89dbb654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/30 00:05:58 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/01/30 00:06:06 WARN MemoryStore: Not enough space to cache rdd_119_2 in memory! (computed 17.0 MiB so far)\n",
      "24/01/30 00:06:06 WARN BlockManager: Persisting block rdd_119_2 to disk instead.\n",
      "24/01/30 00:06:06 WARN MemoryStore: Not enough space to cache rdd_119_3 in memory! (computed 17.0 MiB so far)\n",
      "24/01/30 00:06:06 WARN BlockManager: Persisting block rdd_119_3 to disk instead.\n",
      "24/01/30 00:06:06 WARN MemoryStore: Not enough space to cache rdd_119_6 in memory! (computed 17.0 MiB so far)\n",
      "24/01/30 00:06:06 WARN BlockManager: Persisting block rdd_119_6 to disk instead.\n",
      "24/01/30 00:06:06 WARN MemoryStore: Not enough space to cache rdd_119_0 in memory! (computed 17.0 MiB so far)\n",
      "24/01/30 00:06:06 WARN BlockManager: Persisting block rdd_119_0 to disk instead.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Model Training with Class Weights\n",
    "lr = LogisticRegression(labelCol='label', featuresCol='features', weightCol='classWeight')\n",
    "\n",
    "# Train the model\n",
    "model = lr.fit(df_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c517d653",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c5e3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_summary=model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7d31a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.942042962515077"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Overall accuracy of the classification model\n",
    "lr_summary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb658d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98886127698232"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Area under ROC\n",
    "lr_summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb165eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9396133371106759, 0.9445057300204436]\n"
     ]
    }
   ],
   "source": [
    "#Precision of both classes\n",
    "print(lr_summary.precisionByLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddd19e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9449420483487211, 0.9391375101708698]\n"
     ]
    }
   ],
   "source": [
    "#Recall of both classes\n",
    "print(lr_summary.recallByLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5c19a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Preditions\n",
    "predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a83c7146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 164:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select('label','prediction').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1946fa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/30 00:07:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/01/30 00:07:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 165:=====>                                                  (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1|   2068|\n",
      "|    0|1589573|\n",
      "+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 165:================>                                       (3 + 7) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769d789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
